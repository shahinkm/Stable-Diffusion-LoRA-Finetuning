{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src import AutoStableDiffusionModel, LoraWrapper, convert_to_lora_target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dash_line(length=100):\n",
    "    print(\"-\" * length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_params(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    \n",
    "    print(f\"Trainable model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\nPercentage of trainable parameters: {trainable_model_params/all_model_params*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lora_params_and_config(base_model_name, unet_config, text_encoder_config):\n",
    "    base_model = AutoStableDiffusionModel.from_pretrained(base_model_name)\n",
    "    \n",
    "    if unet_config[\"r\"] != 0:\n",
    "        unet_lora_config = LoraConfig(\n",
    "            r=unet_config[\"r\"],\n",
    "            lora_alpha=unet_config[\"alpha\"],\n",
    "            target_modules=convert_to_lora_target_names(unet_config[\"targets\"], \"unet\"),\n",
    "            init_lora_weights=\"gaussian\",\n",
    "        )\n",
    "    else:\n",
    "        unet_lora_config = None\n",
    "    \n",
    "    if text_encoder_config[\"r\"] != 0:\n",
    "        te_lora_config = LoraConfig(\n",
    "            r=text_encoder_config[\"r\"],\n",
    "            lora_alpha=text_encoder_config[\"alpha\"],\n",
    "            target_modules=convert_to_lora_target_names(text_encoder_config[\"targets\"], \"text_encoder\"),\n",
    "            init_lora_weights=\"gaussian\",\n",
    "        )\n",
    "    else:\n",
    "        te_lora_config = None\n",
    "    \n",
    "    sdxl_lora_model = LoraWrapper.from_config(base_model, unet_lora_config, te_lora_config)\n",
    "    \n",
    "    print(\"Configuration:\")\n",
    "    if unet_lora_config:\n",
    "        print(f\"UNet - r: {unet_config['r']}, alpha: {unet_config['alpha']}, targets: {unet_config['targets']}\")\n",
    "    else:\n",
    "        print(\"UNet - LoRA not applied\")\n",
    "        \n",
    "    if te_lora_config:\n",
    "        print(f\"Text Encoder - r: {text_encoder_config['r']}, alpha: {text_encoder_config['alpha']}, targets: {text_encoder_config['targets']}\")\n",
    "    else:\n",
    "        print(\"Text Encoder - LoRA not applied\")\n",
    "    \n",
    "    print(\"\\nParameters Detail:\")\n",
    "    print_number_of_trainable_model_params(sdxl_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl_base_model_name = \"stabilityai/stable-diffusion-xl-base-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl_experiment1_configs = [\n",
    "    {\n",
    "        \"unet\": {\"r\": 2, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]},\n",
    "        \"text_encoder\": {\"r\": 0, \"alpha\": 0, \"targets\": []}\n",
    "    },\n",
    "    {\n",
    "        \"unet\": {\"r\": 1, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]},\n",
    "        \"text_encoder\": {\"r\": 4, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]}\n",
    "    },\n",
    "    {\n",
    "        \"unet\": {\"r\": 0, \"alpha\": 0, \"targets\": []},\n",
    "        \"text_encoder\": {\"r\": 8, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Configuration 1\n",
      "Configuration:\n",
      "UNet - r: 2, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "Text Encoder - LoRA not applied\n",
      "\n",
      "Parameters Detail:\n",
      "Trainable model parameters: 2903040\n",
      "All model parameters: 3471740907\n",
      "Percentage of trainable parameters: 0.08%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Configuration 2\n",
      "Configuration:\n",
      "UNet - r: 1, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "Text Encoder - r: 4, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "\n",
      "Parameters Detail:\n",
      "Trainable model parameters: 3057152\n",
      "All model parameters: 3471895019\n",
      "Percentage of trainable parameters: 0.09%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Configuration 3\n",
      "Configuration:\n",
      "UNet - LoRA not applied\n",
      "Text Encoder - r: 8, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "\n",
      "Parameters Detail:\n",
      "Trainable model parameters: 3211264\n",
      "All model parameters: 3472049131\n",
      "Percentage of trainable parameters: 0.09%\n"
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(sdxl_experiment1_configs):\n",
    "    print_dash_line()\n",
    "    print(f\"Configuration {i + 1}\")\n",
    "    \n",
    "    unet_config = config[\"unet\"]\n",
    "    text_encoder_config = config[\"text_encoder\"]\n",
    "    \n",
    "    print_lora_params_and_config(sdxl_base_model_name, unet_config, text_encoder_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl_experiment2_configs = [\n",
    "    {\n",
    "        \"unet\": {\"r\": 1, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]},\n",
    "        \"text_encoder\": {\"r\": 0, \"alpha\": 0, \"targets\": []}\n",
    "    },\n",
    "    {\n",
    "        \"unet\": {\"r\": 2, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]},\n",
    "        \"text_encoder\": {\"r\": 0, \"alpha\": 0, \"targets\": []}\n",
    "    },\n",
    "    {\n",
    "        \"unet\": {\"r\": 4, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]},\n",
    "        \"text_encoder\": {\"r\": 0, \"alpha\": 0, \"targets\": []}\n",
    "    },\n",
    "    {\n",
    "        \"unet\": {\"r\": 8, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]},\n",
    "        \"text_encoder\": {\"r\": 0, \"alpha\": 0, \"targets\": []}\n",
    "    },\n",
    "    {\n",
    "        \"unet\": {\"r\": 64, \"alpha\": 1, \"targets\": [\"k\", \"q\", \"v\", \"o\"]},\n",
    "        \"text_encoder\": {\"r\": 0, \"alpha\": 0, \"targets\": []}\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Configuration 1\n",
      "Configuration:\n",
      "UNet - r: 1, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "Text Encoder - LoRA not applied\n",
      "\n",
      "Parameters Detail:\n",
      "Trainable model parameters: 1451520\n",
      "All model parameters: 3470289387\n",
      "Percentage of trainable parameters: 0.04%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Configuration 2\n",
      "Configuration:\n",
      "UNet - r: 2, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "Text Encoder - LoRA not applied\n",
      "\n",
      "Parameters Detail:\n",
      "Trainable model parameters: 2903040\n",
      "All model parameters: 3471740907\n",
      "Percentage of trainable parameters: 0.08%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Configuration 3\n",
      "Configuration:\n",
      "UNet - r: 4, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "Text Encoder - LoRA not applied\n",
      "\n",
      "Parameters Detail:\n",
      "Trainable model parameters: 5806080\n",
      "All model parameters: 3474643947\n",
      "Percentage of trainable parameters: 0.17%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Configuration 4\n",
      "Configuration:\n",
      "UNet - r: 8, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "Text Encoder - LoRA not applied\n",
      "\n",
      "Parameters Detail:\n",
      "Trainable model parameters: 11612160\n",
      "All model parameters: 3480450027\n",
      "Percentage of trainable parameters: 0.33%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Configuration 5\n",
      "Configuration:\n",
      "UNet - r: 64, alpha: 1, targets: ['k', 'q', 'v', 'o']\n",
      "Text Encoder - LoRA not applied\n",
      "\n",
      "Parameters Detail:\n",
      "Trainable model parameters: 92897280\n",
      "All model parameters: 3561735147\n",
      "Percentage of trainable parameters: 2.61%\n"
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(sdxl_experiment2_configs):\n",
    "    print_dash_line()\n",
    "    print(f\"Configuration {i + 1}\")\n",
    "    \n",
    "    unet_config = config[\"unet\"]\n",
    "    text_encoder_config = config[\"text_encoder\"]\n",
    "    \n",
    "    print_lora_params_and_config(sdxl_base_model_name, unet_config, text_encoder_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
